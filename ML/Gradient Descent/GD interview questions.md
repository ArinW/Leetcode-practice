Conceptual
What is gradient descent, and why is it used in machine learning?
Difference between batch, stochastic, and mini-batch gradient descent.
Why is learning rate important? What happens if it’s too high or too low?
How does gradient descent differ from second-order optimization methods? (less common, but nice to know for advanced roles)
Mathematical/Technical
Derive the gradient descent update rule for linear regression with MSE loss.
Apply gradient descent to logistic regression.
Guarantees of convergence in convex problems.
Role of gradient direction and magnitude in updates.
Applied / Practical
How to select/tune learning rate in practice.
Methods to speed up convergence (momentum, RMSProp, Adam).
Handling saddle points / local minima.
Exploding & vanishing gradients.
Advanced / Deep Learning–Focused
Why gradient descent can fail in non-convex optimization (deep nets).
Compare optimizers (Adam, AdaGrad, RMSProp).
Gradient clipping and its role in RNN training.
